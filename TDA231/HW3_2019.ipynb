{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ledX1hQo5kXC"
   },
   "source": [
    "\n",
    "$\\qquad$ $\\qquad$$\\qquad$  **TDA 231 Machine Learning: Home Assignment 3 -- Classification** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Goal: K-Nearest-Neighbour, Naive-bayes Classifier, Support Vector Machine, Logistic Regression**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Yuchong, Divya** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                     **Due Date: 9th May** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Name, Personal No., Email** <br />\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "General guidelines:\n",
    "*   All solutions to theoretical and pratical problems must be submitted in this ipynb notebook, and equations wherever required, should be formatted using LaTeX math-mode.\n",
    "*   All discussion regarding practical problems, along with solutions and plots should be specified in this notebook. All plots/results should be visible such that the notebook do not have to be run. But the code in the notebook should reproduce the plots/results if we choose to do so.\n",
    "*   Your name, personal number and email address should be specified above.\n",
    "*   All tables and other additional information should be included in this notebook.\n",
    "*   **All the answers for theoretical questions must be filled in the cells created for you with \"Your answer here\" below each question, but feel free to add more cells if needed.**\n",
    "*   Before submitting, make sure that your code can run on another computer. That all plots can show on another computer including all your writing. It is good to check if your code can run here: https://colab.research.google.com.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fbqQD9bj6HzP"
   },
   "source": [
    "\n",
    "# Theoretical Questions\n",
    "## 1. K-Nearest-Neighbour Classification (4 pts)\n",
    "### 1.1 Exercise 1 (2 pts)\n",
    "A KNN classifier assigns a test instance the majority class associated with its K nearest training instances. Distance between instances is measured using Euclidean distance. Suppose we have the following training set of positive (+) and negative (-) instances and a single test instance (o). All instances are projected onto a vector space of two real-valued features (X and Y). Answer the following questions. Assume “unweighted” KNN (every nearest neighbor contributes equally to the final vote).\n",
    "\n",
    "![替代文字](https://raw.githubusercontent.com/BruceZHANG6/Imagesforuse/master/knn.png)\n",
    "\n",
    "a) What would be the class assigned to this test instance for K=1, K=3, K=5 and why? (**1 pt**)\n",
    "\n",
    "b) What will be the maxinum value of K you think in this case? Why? (**1 pt**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K0xnH14a0rsj"
   },
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FqgZRVS80xr7"
   },
   "source": [
    "### 1.2 Exercise 2 (2 pts)\n",
    "Consider 5 data points:\n",
    "\n",
    "$$\\{({0},{1}), ({-1},{0})\\}∈ Class1,$$ \n",
    "\n",
    "$$\\{({1},{0}), ({0},{-1}), (-\\frac{1}{2}, \\frac{1}{2})\\}∈ Class2.$$\n",
    "\n",
    "Consider two test data points:\n",
    "\n",
    "$$(-\\frac{3}{4}, \\frac{3}{4})∈ Class1, (\\frac{1}{2}, \\frac{1}{2})∈ Class2$$\n",
    "\n",
    "Compute the probability of error based on k-nearest neighbor rule when $ K=\\{1, 2, 3, 4, 5\\}$ and explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pK-7HuAB0ztS"
   },
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8t9u_kDpgTD2"
   },
   "source": [
    "## 2. [Naive Bayes Classifier, 6 points]\n",
    "\n",
    "### Exercise 1 (3 pts)\n",
    "A psychologist does a small survey on ''happiness''. Each respondent provides a vector with entries 1 or 0 corresponding to if they answered “yes” or “no” to a question respectively. The question vector has attributes \n",
    "$$\n",
    "x = (\\mbox{rich, married, healthy}) \\tag{1}\n",
    "$$\n",
    "\n",
    "Thus a response $(1, 0, 1)$ would indicate that the respondent was\n",
    "''rich'', ''unmarried'' and ''healthy''. In addition, each respondent\n",
    "gives a value $c = 1$ if they are content wih their life and $c = 0$\n",
    "if they’re not. The following responses were obtained.\n",
    "\n",
    "$$\n",
    "c = 1: (1, 1, 1),(0, 0, 1),(1, 1, 0),(1, 0, 0) \\\\\n",
    "c = 0: (0, 0, 0),(1, 0, 1),(0, 0, 1),(0, 1, 0)\n",
    "$$\n",
    "\n",
    "1. Using naive Bayes, what is the probability that a person is ''not rich'', ''married'' and ''healthy'' is ''content''? **(1 pt)**\n",
    "\n",
    "2. What is the probability that a person who is ''not rich'' and ''married'' is content ? (i.e. we do not know if they are ''healthy'') **(2 pts)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEEeDnaN1Ikp"
   },
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FkqFQkN1LWx"
   },
   "source": [
    "### Exercise  2 (3 pts)\n",
    "We now consider real valued data i.e. $x \\in \\mathbb{R}^2$.\n",
    "#### a. (1 pt)\n",
    "Assume that the class conditional density is **spherical** Gaussian, and both classes have equal fixed prior i.e. $p(y= \\pm 1) = 0.5$. Write the expression for the **naive Bayes** classifier i.e. derive\n",
    "$$\n",
    "P(y_{new} = -1 | x_{new} , X, y ) \\\\ \\tag{1}\n",
    "P(y_{new} = +1 | x_{new} , X, y ) ~.\n",
    "$$\n",
    "\n",
    "It is useful to note that the dependence on training data $X, y$ for class $i$ can be expressed as: \n",
    "\n",
    "$$ \n",
    "P( x_{new} | y_{new} = 1, X, y) = P(x_{new} |\n",
    "\\mu_{i}, \\sigma^{2}_{i})\n",
    "$$\n",
    "\n",
    "where $\\mu_{i} \\in \\mathbb{R}^2$ and $\\sigma^{2}_{i}\\in \\mathbb{R}$.\n",
    "\n",
    "***Hint***: Derive the expressions of MLE for parameters in terms of training-data. Then express eq.1 in terms of those estimates. \n",
    "\n",
    "#### b. (2 pts)\n",
    "Derive the MLE expression for parameters when the covariance matrix is not diagonal, i.e, Covariance matrix has 4 unknown parameters. This is done to alleviate \"naive\" assumption, since now feature components are no longer independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nyg8jlaj1Nb8"
   },
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_3iVQypENwJQ"
   },
   "source": [
    "## 3. [SVM, 5 points]\n",
    "\n",
    "### a (2 pts)\n",
    "\n",
    "Consider a (hard margin) SVM with the following training points from\n",
    "two classes:\n",
    "\\begin{eqnarray}\n",
    "+1: &(2,2), (4,4), (4,0) \\nonumber \\\\\n",
    "-1: &(0,0), (2,0), (0,2) \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "1. Plot these six training points, and construct, by inspection, the\n",
    "weight vector for the optimal hyperplane. **(1 pt)**\n",
    "\n",
    "2. In your solution, specify the hyperplane in terms of w and b such that $w_1 x_1 + w_2 x_2 + b =0$. Calculate the margin, i.e. $2\\gamma$, where $\\gamma$ is the\n",
    "distance from the hyperplane to its closest data point. (Hint: It may be useful to recall that the distance of a point $(a_1,a_2)$ from the line $w_1x_1 + w_2x_2 + b = 0$ is $|w_1a_1 + w_2a_2 + b|/\\sqrt{w_1^2 + w_2^2}$.) **(1 pt)**\n",
    "\n",
    "### b (3 pts)\n",
    "\n",
    "Consider the same problem from above.\n",
    "\n",
    "1. Write the primal formulation of the SVM **for this specific example** i.e. you have to specialise the general formulation for the set of inputs given. **(1 pt)**\n",
    "\n",
    "2. Write the dual formulation **for this specific**. Give the optimal dual solution, comment on support vectors. **(2 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZWt7B2y1Qqx"
   },
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhHF2_-R00si"
   },
   "source": [
    "# Practical Question\n",
    "## 4. Logistic Regression (5 pts)\n",
    "### Customer churn with Logistic Regression\n",
    "A telecommunications company is concerned about the number of customers leaving their land-line business for cable competitors. They need to understand who is leaving. Imagine that you are an analyst at this company and you have to find out who is leaving and why.\n",
    "\n",
    "### About the dataset\n",
    "\n",
    "We will use a telecommunications dataset for predicting customer churn. This is a historical customer dataset where each row represents one customer. The data is relatively easy to understand, and you may uncover insights you can use immediately. Typically it is less expensive to keep customers than acquire new ones, so the focus of this analysis is to predict the customers who will stay with the company.\n",
    "This data set provides information to help you predict what behavior will help you to retain customers. You can analyse all relevant customer data and develop focused customer retention programs.\n",
    "The dataset includes information about:\n",
    "*   Customers who left within the last month – the column is called Churn.\n",
    "*   Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies.\n",
    "*   Customer account information – how long they had been a customer, contract, payment method, paperless billing, monthly charges, and total charges.\n",
    "*   Demographic info about customers – gender, age range, and if they have partners and dependents.\n",
    "We will help you load and visualise the dataset as well as the preprocessing, you need to build up your logistic regression model step by step and do the prediction.\n",
    "*   **Remember, you are not allowed to use sklearn in modelling and predicting, you have to fill your code in the skeleton.** \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QR5cEzzzOVba"
   },
   "outputs": [],
   "source": [
    "## Load the dataset and read it\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "import urllib\n",
    "\n",
    "try:\n",
    "    urllib.request.urlretrieve('https://raw.githubusercontent.com/simonpf/tda231/master/assignment_3/ChurnData.csv?token=Ah51rRAYro9lVcOFX9sJRtUQNgaziTO4ks5cvuGdwA%3D%3D', 'ChurnData.csv')\n",
    "except urllib.error.HTTPError as ex:\n",
    "    print('Problem:', ex)\n",
    "    \n",
    "churn_df = pd.read_csv(\"ChurnData.csv\")\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "s2gDgP27WNFV"
   },
   "outputs": [],
   "source": [
    "## Data pre-processing and selection\n",
    "## Train/Test dataset split\n",
    "churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']]\n",
    "churn_df['churn'] = churn_df['churn'].astype('int')\n",
    "churn_df.head()\n",
    "\n",
    "X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']])\n",
    "y = np.asarray(churn_df['churn'])\n",
    "y = np.reshape(y, (np.asarray(churn_df['churn']).shape[0], 1))\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\n",
    "X_train=X_train.T\n",
    "X_test=X_test.T\n",
    "y_train=y_train.T\n",
    "y_test=y_test.T\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMzOgQsCoJSN"
   },
   "source": [
    "**Hints**:\n",
    "- You compute $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$ .\n",
    "- You compute activation $A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$.\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$.\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$\n",
    "\n",
    "- You write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate.\n",
    "- In prediction, you calculate $\\hat{Y} = A = \\sigma(w^T X + b)$.\n",
    "- You may use np.exp, np.log(), np.dot(), etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NCZ4BQ-uW6hg"
   },
   "outputs": [],
   "source": [
    "## Modeling and predicting\n",
    "\n",
    "# GRADED FUNCTION: sigmoid\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Return: s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = #Your code here\n",
    "    \n",
    "    return s\n",
    "  \n",
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized to 0\n",
    "    \"\"\"    \n",
    "    w = #Your code here\n",
    "    b = #Your code here\n",
    "   \n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b\n",
    "  \n",
    "# GRADED FUNCTION: grad_cost\n",
    "def grad_cost(w, b, X, Y):\n",
    "  \"\"\"\n",
    "    Arguments:\n",
    "    X -- data of size (number of features, number of examples)\n",
    "    Y -- true \"label\" vector\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "  \"\"\" \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    A = #Your code here                                   \n",
    "    cost = #Your code here                         \n",
    "    dw = #Your code here\n",
    "    db = #Your code here\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    " # GRADED FUNCTION: optimize\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = grad_cost(w,b,X,Y)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule\n",
    "        w = #Your code here\n",
    "        b = #Your code here\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)     \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}   \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "  \n",
    "# GRADED FUNCTION: predict\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    A = #Your code here\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        if A[0,i]<=0.5:\n",
    "            Y_prediction[0,i]=0;\n",
    "        if A[0,i]>0.5:\n",
    "            Y_prediction[0,i]=1\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction\n",
    "  \n",
    "# GRADED FUNCTION: model\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    Returns: d -- dictionary containing information about the model.\n",
    "    \"\"\"    \n",
    "    # initialize parameters with zeros\n",
    "    w, b = #Your code here\n",
    "\n",
    "    # Gradient descent\n",
    "    parameters, grads, costs = #Your code here\n",
    "    \n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    Y_prediction_test = predict(w,b,X_test)\n",
    "    Y_prediction_train = predict(w,b,X_train)\n",
    "\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "   \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wl7F4KSwZo9T"
   },
   "outputs": [],
   "source": [
    "## The train accuracy and test accuracy\n",
    "## Feel free to change the hyperparameters\n",
    "d = model(X_train, y_train, X_test, y_test, num_iterations = 20000, learning_rate = 0.005, print_cost = True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW3_2019.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
