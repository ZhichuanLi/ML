{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$ $\\qquad$$\\qquad$  **TDA 231 Machine Learning: Homework 4, part 1** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Goal: Fully-connected deep neural networks**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Emilio, Simon** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                     **Due Date: 21/5** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Name, Personal no., email** <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General guidelines:\n",
    "* All solutions to theoretical and practical problems should be submitted in this Jupyter notebook.\n",
    "* All discussion regarding practical problems, along with solutions and plots should be specified in this notebook. All plots/results should be visible such that the notebook do not have to be run. But the code in the notebook should reproduce the plots/results if we choose to do so.  \n",
    "* Your name, personal number and email address should be specified above.\n",
    "\n",
    "**Jupyter/IPython Notebook** is a collaborative Python web-based environment. This will be used in all our Homework Assignments. It is installed in the halls ES61-ES62, E-studio and MT9. You can also use google-colab: https://colab.research.google.com\n",
    "to run these notebooks without having to download, install, or do anything on your own computer other than a browser.\n",
    "Some useful resources:\n",
    "\n",
    "1. https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/ (Quick-start guide)\n",
    "2. https://www.kdnuggets.com/2016/04/top-10-ipython-nb-tutorials.html\n",
    "3. http://data-blog.udacity.com/posts/2016/10/latex-primer/ (latex-primer)\n",
    "4. http://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html (markdown)\n",
    "\n",
    "In this assignment you will be using the `pytorch` package. Installation instructions can be found on the [pytorch homepage](https://pytorch.org/get-started/locally/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this assignment, you will be working extensively with vectors, matrices and tensors. A rank-$k$ tensor with shape\n",
    "$(n_1, \\ldots, n_k)$ can be defined as a collection of scalar variables $X_{i_1, \\ldots, i_k} \\in \\mathrm{R}$ where\n",
    "each of the indices $i_j, j \\in [1, k]$ can take values in the range $[1, n_j]$.\n",
    "\n",
    "You can think of a tensor as a mathematical representation of a multi-dimensional array. This multi-dimensional array\n",
    "may hold for example a collection of input images that we want to feed into a neural network. In the case of\n",
    "grey-scale images, these could be represented by a rank-3 tensor with the index $i_1$ representing the different\n",
    "images in the collection, and $i_2, i_3$ the vertical and horizontal dimensions along each image. \n",
    "\n",
    "Note how vectors and matrices can be obtained from this definition as special cases for $k = 1$ and $k = 2$, respectively.\n",
    "\n",
    "# Notation\n",
    "\n",
    "We will use regular letters for scalar variables, bold letters for vector-valued variables, e.g. $\\mathbf{v} \\in \n",
    "\\mathrm{R}^n$, and bold, capital letters for matrices and higher-dimensional tensors,\n",
    "e.g. $\\mathbf{W}$ for a matrix $\\mathbf{W} \\in \\mathrm{R}^{m \\times n}$. The scalar  elements of vectors\n",
    "and matrices  will be written using regular letters and regular, capital letters, respectively:\n",
    "\n",
    "$$\n",
    "v_i \\in \\mathrm{R}, X_{i,j} \\in \\mathrm{R}\n",
    "$$\n",
    "\n",
    "If a subscript index is used to distinguish vector- or matrix-valued variables, we will use a colon to separate the\n",
    "variable index and element indices:\n",
    "\n",
    "- $v_{0:i}$ denotes the $i$th element of the vector $\\mathbf{v}_0$ \n",
    "- $X_{0:i,j}$ denotes $j$th element of the $i$th row of the matrix $\\mathbf{X}_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Deep learning applications, 1 point]\n",
    "\n",
    "Describe an application where deep learning is used in the real world.\n",
    "Provide a reference to support your example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Backpropagation by hand, 1 point]\n",
    "\n",
    "Consider the simple feed-forward neural network depicted in the figure below. This network\n",
    "consists of a three-dimensional input layer $\\mathbf{y}_0 = \\mathbf{x}$,  one hidden layer\n",
    "with activations $\\mathbf{y}_1$ and a one-dimensional output layer with activations $\\mathbf{y}_2$.\n",
    "\n",
    "![](simple_nn.png)\n",
    "\n",
    "\n",
    "The activations of a layer $i$ are computed by applying a linear transformation given by the weight matrix\n",
    "$\\mathbf{W}_i$ to the input activations $\\mathbf{y}_{i - 1}$ producing the intermediate values $\\mathbf{z}_i$:\n",
    "\n",
    "$$\n",
    "z_{i : j} = \\sum_k W_{i:j, k} y_{i - 1:k} \\\\\n",
    "$$\n",
    "\n",
    "This is followed by the element-wise application of the layers'\n",
    "activation function $f_i$ to the intermediate values $\\mathbf{z}_i$:\n",
    "\n",
    "$$\n",
    "y_{i:j} = f_i (z_{i:j})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1, a)\n",
    "\n",
    "Given the derivative of an error term $E$ with respect to the activation of the output neuron \n",
    "$\\frac{dE}{dy_{2:0}}$, derive expressions for the derivatives of the error term with respect to the weights\n",
    "$W_{i:j,k}$ and activations $y_{i:j}$ of the remaining layers of the network.\n",
    "\n",
    "To simplify your results, you are encouraged to reuse already computed derivative terms in the computation of\n",
    "downstream derivatives.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dE}{dW_{2:i, j}} = \\: ? \\\\\n",
    "\\frac{dE}{dy_{1:j}} = \\: ? \\\\\n",
    "\\frac{dE}{dW_{1:i, j}} = \\: ? \\\\\n",
    "\\frac{dE}{dy_{0:j}} = \\: ? \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If your calculations are correct, you should see that you can express the derivatives of the error function \n",
    "around a given layer in the network using the derivatives from the next higher layer. This yields a simple\n",
    "recipe to successively compute the gradients in a feed forward neural network by starting at the last layer and\n",
    "then computing the gradients layer-by-layer as you move backwards through the network. This method is commonly\n",
    "referred to as **backpropagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Chain rule for tensors, 6 points]\n",
    "\n",
    "\n",
    "In principle, the results from the previous exercise are sufficient to implement back propagation to train a neural\n",
    "network using stochastic gradient descent, that is if the network is trained one sample at a time. In reality,\n",
    "however, it is desirable to be able to train neural networks on batches of input vectors as well as higher-dimensional inputs, such as images. A generalized form of the back propagation algorithm for tensor-valued input can be obtained from the chain rule for tensor functions, which you will derive in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2, a)\n",
    "\n",
    "Consider the following two functions:\n",
    "\n",
    "-  A tensor-valued function \n",
    "$\\mathbf{u}: \\mathrm{R}^{n_{1} \\times \\ldots \\times n_{k}} \\rightarrow \\mathrm{R}^{m_1 \\times \\ldots \\times m_l}$\n",
    "mapping a $k$-Tensor $\\mathbf{X} = X_{i_1,\\ldots, i_{k}}$ to a $l$-Tensor\n",
    "$\\mathbf{u}(\\mathbf{X}) = u_{j_1, \\ldots j_{l}}(\\mathbf{X})$ \n",
    "\n",
    "- A scalar-valued function $v : \\mathrm{R}^{m_1 \\times \\ldots \\times m_l} \\rightarrow \\mathrm{R}$ that maps\n",
    "  a $l$-Tensor $\\mathbf{X} = X_{i_1,\\ldots, i_l}$  to a scalar value $v(\\mathbf{X})$\n",
    "  \n",
    "Derive an expression for the gradient tensor of the composition  $w(\\mathbf{X})  = v(\\mathbf{u}(\\mathbf{X}))$\n",
    "of the two functions $\\mathbf{u}$ and $v$ in terms of their respective gradient tensors\n",
    "$\\frac{dv(\\mathbf{X})}{dx_{i_1, \\ldots, i_l}} = \\frac{dv(\\mathbf{X})}{d\\mathbf{X}}$ and\n",
    "$\\frac{du_{j_1, \\ldots, j_l}(\\mathbf{X})}{dx_{i_1, \\ldots, i_k}} = \\frac{d\\mathbf{u}(\\mathbf{X})}{d\\mathbf{X}}$:\n",
    "\n",
    "$$\n",
    "\\frac{dw(\\mathbf{X})}{dx_{i_1, \\ldots, i_k}} = \\: ?\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2, b)\n",
    "\n",
    "Now consider a layer in a feed forward neural network for which we want to\n",
    "compute activations and gradients for a batch of input activations. In this\n",
    "case the input activations can be represented using a matrix\n",
    "$\\mathbf{Y}_{i - 1} \\in \\mathrm{R}^{n \\times k}$,\n",
    "where $n$ is the number of input features of the layer and $k$ the number of\n",
    "samples in the batch.\n",
    "\n",
    "The layer can be viewed as the composition of a linear transformation\n",
    "$\\mathbf{u}(\\mathbf{Y}, \\mathbf{W}) = \\mathbf{W}\\mathbf{Y} = \\sum_k W_{i, k} Y_{k ,j}$\n",
    "and the element-wise application of the activation function\n",
    "$\\mathbf{v}(\\mathbf{X}) = f_i(X_{k, l})$.\n",
    "\n",
    "Compute the gradient tensors of the functions $\\mathbf{u}$ and $\\mathbf{v}$ for this layer. What is special about the structure of\n",
    "the tensors?\n",
    "\n",
    "$$\n",
    "\\frac{dv_{i_1, i_2}(\\mathbf{Z})}{dZ_{i_3, i_4}} =  \\: ? \\\\\n",
    "\\frac{du_{i_1, i_2}(\\mathbf{Y}, \\mathbf{W})}{dY_{i_3, i_4}} = \\: ? \\\\\n",
    "\\frac{du_{i_1, i_2}(\\mathbf{Y}, \\mathbf{W})}{dW_{i_3, i_4}} = \\: ? \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2, c)\n",
    "\n",
    "We could now apply the chain rule for tensor functions derived in **a)** to obtain the backward propagation rule to\n",
    "compute the weight and input-activation gradients  from the error-function gradient of the output\n",
    "activations of the layer. However, straight-forward application of the rule would yied products involving four\n",
    "dimensional gradient tensors. A computationally much more efficient formulation using only matrix-matrix products and\n",
    "element-wise products of matrices can be obtained when the special structure of the gradients derived in the steps\n",
    "above is taken into account.\n",
    "\n",
    "Derive expressions for the weight- and input-activation gradients, $\\frac{dE}{d\\mathbf{W}_{i}}$ and\n",
    "$\\frac{dE}{d\\mathbf{Y}_{i-1}}$, from the incoming gradient $\\frac{dE}{d\\mathbf{Y}_i}$ of the error\n",
    "$E$ with respect to the output activation of the layer. You should only use matrix-matrix and\n",
    "element-wise multiplication of matrices. Use $\\odot$ to write the element-wise products of matrices.\n",
    "\n",
    "$$\n",
    "\\frac{dE}{d\\mathbf{W}_i} = \\: ? \\\\\n",
    "\\frac{dE}{d\\mathbf{Y}_{i - 1}} = \\: ?\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Implementing a Fully-connected layer, 4 points]\n",
    "\n",
    "There are two important advantages of the formulation of backprop for fully-connected layers that you derived above:\n",
    "\n",
    "- The formulation is independent of batch size. That means it works for a single input sample as well as a batch\n",
    "  containing several hundreds of them.\n",
    "- The forward and backward operations can be implemented using matrix operations. Efficient implementations of these\n",
    "  are available in basically every programming language and are often already parallelized.\n",
    "\n",
    "This allows us to write our own implementation of a dense layer in numpy that can be applied to realistic data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a)\n",
    "\n",
    "Implement the forward and backward member functions of the `CustomLayer` class defined below so that it implements a fully-connected neural network layer with ReLU activation functions:\n",
    "\n",
    "$$\n",
    "\\mathbf{Y}_{i} = f(\\mathbf{W_i} \\mathbf{Y}_{i - 1}) \\\\\n",
    "f(x) = \\begin{cases} x & \\text{if } x \\geq 0 \\\\ 0 & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "    \n",
    "For the sake of computational efficiency, you are not allowed to use any explicit loops. All necessary operations\n",
    "can be implemented using the operators `*` and `@` for element-wise and matrix-matrix products together with a few\n",
    "more numpy built-in functions. Check the [numpy documentation](https://docs.scipy.org/doc/numpy-1.14.0/reference/arrays.ndarray.html) for a list functions that are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CustomLayer:\n",
    "    \"\"\"\n",
    "    The CustomLayer class implements a fully connected layer with ReLU activation\n",
    "    funtions.\n",
    "    \n",
    "    Attributes:\n",
    "        \n",
    "        w(numpy.ndarray): The weight matrix defining the strengths of the\n",
    "            connections between input and output neurons of the layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, w):\n",
    "        \"\"\"\n",
    "        Create a CustomLayer instance with weight matrix w.\n",
    "        \"\"\"\n",
    "        self.w = w\n",
    "        \n",
    "    def forward(self, y_in):\n",
    "        \"\"\" Forward propagate the input activations through the layer.\n",
    "        \n",
    "        Args:\n",
    "            y_in(numpy.ndarray): Vector of input activations\n",
    "            \n",
    "        Returns:\n",
    "            y_out(numpy.ndarray): Output activations of the layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        z = ...\n",
    "        y_out = ...\n",
    "        \n",
    "        # Note that you also need to store the input activations\n",
    "        # and the intermediate values z since the are needed\n",
    "        # in the computation of the backward step.\n",
    "        self.y_in = y_in\n",
    "        self.z = z\n",
    "        \n",
    "        return y_out\n",
    "    \n",
    "    def backward(self, dedy_out):\n",
    "        \"\"\" Backward propagate gradients of loss function through the layer.\n",
    "        \n",
    "        Args:\n",
    "            dedy_out(np.ndarray): Gradients of loss function w.r.t. output\n",
    "                activations.\n",
    "        Returns:\n",
    "            (dedy_in, dedw): Tuple containing the loss gradients w.r.t to input\n",
    "                activations (dedy_in) and the weight matrix of the layer (dedw).\n",
    "        \"\"\"\n",
    "        \n",
    "        dedz = ...\n",
    "        dedy = ...\n",
    "        dedw = ...\n",
    "        \n",
    "        return dedy, dedw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3, b) Running your code\n",
    "\n",
    "Now use your `CustomLayer` class to propagate input activations forward and input gradients backward through the network. Complete the code below. Note that it is important the you use the predefined names for the output variables as the code further down reuses these\n",
    "values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_in = np.random.normal(size = (2, 3))\n",
    "w    = np.random.normal(size = (3, 2))\n",
    "\n",
    "# Propagate the input activations y_in through the\n",
    "# custom layer and store output activations in y_out.\n",
    "y_out = ...\n",
    "\n",
    "# Now propagate a matrix containing dummy gradients backward\n",
    "# through the layer.\n",
    "dedy_out = np.ones((..., ...))\n",
    "dedy_in, dedw = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3, c) Computing gradients with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course there are much more powerful tools for machine learning  available\n",
    "in Python than `numpy`. One of them is `pytorch`, which is commonly used in machine\n",
    "learning research.\n",
    "\n",
    "One of the core features of `pytorch` is its `tensor` class. `pytorch` tensors are\n",
    "multi-dimensional arrays on which you can perform\n",
    "arithmetic operations. `pytorch` keeps track of the functions applied to\n",
    "each tensor so that gradients can be backpropagated from tensors\n",
    "that are results of functions applied to other tensors. \n",
    "The gradient tensors of the tensors involved in the computation can be accessed\n",
    "as the `grad` attribute of each tensor object.\n",
    "\n",
    "The example below shows how to compute the gradient of the mean taken over the\n",
    "element of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# The requires_grad keyword tells torch to keep track of the gradients.\n",
    "u = torch.ones((2, 2), requires_grad = True)\n",
    "v = torch.mean(u)\n",
    "# For scalar values backward can be called without input argument.\n",
    "v.backward()\n",
    "u.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3, d) Testing your custom layer\n",
    "\n",
    "As a next step, you will validate your backprop implementation for the custom\n",
    "layer by comparing the computed gradients with gradients computed using `pytorch`.\n",
    "\n",
    "`pytorch` tensors can be created from numpy arrays using the `torch.tensor`\n",
    "function. By passing the `requires_grad = True` keyword, we tell `pytorch`\n",
    "to keep track of the operations applied to the tensors so that it can compute\n",
    "gradients.\n",
    "\n",
    "Complete the code below be uncommenting the 4 commented lines and add the code\n",
    "that is missing to compute the output activations `y_out_t` of a fully-connected\n",
    "layer with ReLU activations. Check the [pytorch documentation](https://pytorch.org/docs/master/torch.html?#torch.clamp)\n",
    "for a list of available functions on `pytorch` tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "y_in_t = torch.tensor(y_in, requires_grad = True)\n",
    "w_t = torch.tensor(w, requires_grad = True)\n",
    "dedy_out_t = torch.tensor(dedy_out, requires_grad = True)\n",
    "\n",
    "## Forward propagation\n",
    "#y_out_t = ...\n",
    "\n",
    "## Compute gradients\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If both your `CustomLayer` and pytorch implementation are correct,\n",
    "the code below will run without throwing an exception. If it does\n",
    "throw an exception, you probably have a mistake in your\n",
    "implementation of forward or backward propagation.\n",
    "\n",
    "**Note:** There's no point continuing if the asserts don't pass. If your\n",
    "layer does not compute the correct gradients, you cannot train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(np.all(np.isclose(y_out, y_out_t.detach())))\n",
    "assert(np.all(np.isclose(w_t.grad, dedw)))\n",
    "assert(np.all(np.isclose(y_in_t.grad, dedy_in)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Neural network in pytorch, 4 points]\n",
    "\n",
    "\n",
    "The `CustomLayer` class that you implemented above can\n",
    "even be used in a neural network within `pytorch`. Some additional\n",
    "code is required for this and is provided below.\n",
    "It might look intimidating, but most of what it does is make\n",
    "sure that incoming and outgoing pytorch tensors are correctly converted\n",
    "to numpy arrays and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd          import Function\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.parameter      import Parameter\n",
    "\n",
    "class CustomLayerWrapper(CustomLayer, Function):\n",
    "    \"\"\"Wrapper for CustomLayer that implements torch.autograd.function.\n",
    "    \n",
    "    This class makes the CustomLayer class usable with torchs autograd\n",
    "    module for automatic differentiation.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Create layer with weight matrix w.\"\"\"\n",
    "        Function.__init__(self)\n",
    "        \n",
    "    @staticmethod\n",
    "    def forward(ctx, y_in, w):\n",
    "        \"\"\"\n",
    "        pytorch autograd compatible wrapper for the forward member\n",
    "        function of the CustomLayer class.\n",
    "        \"\"\"\n",
    "        ctx.custom_layer = CustomLayer(w.detach().numpy())\n",
    "        y_in = y_in.detach().numpy()\n",
    "        y_out = ctx.custom_layer.forward(y_in.T)\n",
    "        return torch.from_numpy(y_out.T)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, de_dyout):\n",
    "        \"\"\"\n",
    "        pytorch autograd compatible wrapper for the forward member\n",
    "        function of the CustomLayer class.\n",
    "        \"\"\"\n",
    "        dedy_out = de_dyout.detach().numpy()\n",
    "        de_dy, dw_dy = ctx.custom_layer.backward(dedy_out.T)\n",
    "        de_dy = torch.from_numpy(de_dy.T)\n",
    "        dw_dy = torch.from_numpy(dw_dy)\n",
    "        return de_dy, dw_dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4, a) Weight initialization\n",
    "\n",
    "The `CustomLayerModule` class defined below declares the weights of the layer\n",
    "as trainable parameters by instantiating `pytorch`s `Parameter` class. The same line also\n",
    "takes care of initializing the weights of the layer. What is the strategy\n",
    "used for this and why is this important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayerModule(Module):\n",
    "    \"\"\"\n",
    "    Module wrapper for CustomLayer that can be used in a pytorch neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, dtype = torch.float):\n",
    "        \"\"\"\n",
    "        Create a CustomLayer with given input and output dimensions\n",
    "        and initialize weights with random values.\n",
    "        \n",
    "        Args:\n",
    "            input_dim(int): Number of input activations\n",
    "            output_dim(int): Number of output activations\n",
    "            dtype(torch.dtype): Precision type to use for weight matrix.\n",
    "        \"\"\"\n",
    "        Module.__init__(self)\n",
    "        k = np.sqrt(1 / input_dim)\n",
    "        self.w = Parameter(k * torch.randn((output_dim, input_dim)))\n",
    "\n",
    "    def forward(self, y_in):\n",
    "        \"\"\" Forwards call to CustomLayerWrapper. \"\"\"\n",
    "        return CustomLayerWrapper().apply(y_in, self.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4, b)   Defining a neural network in pytorch\n",
    "\n",
    "The code below defines the `SimpleNet` class, which uses our custom layer\n",
    "in a neural network to do 10-class classification.\n",
    "\n",
    "Which acitivation function does the last layer in the network use and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.functional import log_softmax\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple neural network with one hidden layer to perform 10-class\n",
    "    classification suitable for application on the MNIST dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = CustomLayerModule(28 * 28, 64)\n",
    "        self.layer_2  = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Predict classes for the input tensor x.\n",
    "        \n",
    "        Arguments:\n",
    "            x(torch.tensor): Input tensor to classify.\n",
    "            \n",
    "        Returns:\n",
    "            y(torch.tensor): Tensor with the softmax activations\n",
    "                of the output layer.\n",
    "        \"\"\"\n",
    "        x = x.view(x.shape[0], -1) # Flatten input\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        return log_softmax(x, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4,  c) Implementing gradient descent\n",
    "\n",
    "With `pytorch` taking care of the calculation of the gradients,\n",
    "implementing gradient descent to train the network becomes \n",
    "straight forward.\n",
    "\n",
    "Uncomment and complete the last commented line in the code so that the\n",
    "`gradient_descent` method defined below trains a given model using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(model,\n",
    "                     training_loader,\n",
    "                     loss_function,\n",
    "                     learning_rate = 0.001):\n",
    "    \"\"\"\n",
    "    Trains the given model on the training data provided by\n",
    "    training loader using gradient descent\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        model(torch.nn.Module): The pytorch module to be trained\n",
    "        \n",
    "        training_loader(torch.utils.data.Dataloader): Dataloader providing\n",
    "            the training data.\n",
    "            \n",
    "        loss_function: pytorch function to use as training criterion\n",
    "        \n",
    "        learning_rate: The learning rate to apply\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    for _, (data, target) in enumerate(training_loader):\n",
    "        \n",
    "        # pytorch accumulates gradients when computed over several steps. For pure gradient\n",
    "        # descent we therefore need to zero out gradients of all parameters of the model.\n",
    "        for p in model.parameters():\n",
    "            if not p.grad is None:\n",
    "                p.grad.data.zero_()\n",
    "                \n",
    "        # Compute model output (forward propagation)\n",
    "        output = model(data)\n",
    "        \n",
    "        # Compute scalar loss value.\n",
    "        loss = loss_function(output, target)\n",
    "        \n",
    "        # Backpropagate error gradients.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Loop over all learnable parameter tensors p\n",
    "        # and perform gradient descent step.\n",
    "        #\n",
    "        # To manipulate the specific values of the parameter tensors, you\n",
    "        # need to access the data attribute instead of the tensor itself.\n",
    "        # This is because you don't want pytorch to consider this manipulation\n",
    "        # when computing gradients.\n",
    "        for p in model.parameters():\n",
    "            p.data = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data\n",
    "\n",
    "Last but not least, we need data to train the network. The `torchvision` package provides\n",
    "very convenient interfaces for popular benchmark datasets. The code below donloads MNIST training data and creates\n",
    "a data loader that then feeds the data to our training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "\n",
    "transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n",
    "training_data = datasets.MNIST(root = './data', train = True, download = True, transform = transform)\n",
    "\n",
    "n = len(training_data)\n",
    "n_train = int(0.9 * n)\n",
    "n_val = n - n_train\n",
    "training_data, validation_data = random_split(training_data, [n_train, n_val])\n",
    "training_loader   = DataLoader(training_data, batch_size = 256, shuffle = True)\n",
    "validation_loader = DataLoader(validation_data, batch_size = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4, d) Training the classifier\n",
    "\n",
    "Now, putting all pieces together, all that is left to do to train an instance\n",
    "of the `SimpleNet` class is to choose a suitable loss function and run the training.\n",
    "\n",
    "Complete the code below with a suitable loss function. For a list of available loss\n",
    "functions check the [pytorch documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "loss = ...\n",
    "model = SimpleNet()\n",
    "gradient_descent(model, training_loader, loss, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_error(model):\n",
    "    \"\"\"\n",
    "    Compute the validation error of the given model in percent.\n",
    "    \"\"\"\n",
    "    error = 0.0\n",
    "    for (data, target) in validation_loader:\n",
    "        error += torch.sum(model(data).argmax(dim = 1) != target).float()\n",
    "    error /= float(len(validation_data))\n",
    "    return 100.0 * error\n",
    "    \n",
    "print(\"Validation error after 1 epoch: {0:.2f}%.\".format(validation_error(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing outputs\n",
    "\n",
    "To illustrate what our model has learned, we plot predictions on a number of\n",
    "samples from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "f, axs = plt.subplots(2, 4, figsize = (12, 6))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for (data, target), ax in zip(validation_loader, axs):\n",
    "    \n",
    "    # Compute prediction\n",
    "    predicted = model(data).argmax(dim = 1)[0]\n",
    "    target = target[0]\n",
    "    title = \"truth: {0}\\npredicted: {1}\".format(target, predicted)\n",
    "    \n",
    "    # Plot input images.\n",
    "    ax.matshow(data[0, 0, :, :], cmap = \"Greys\")\n",
    "    ax.set_title(title, loc = \"left\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [A better classifier, 2 points]\n",
    "\n",
    "You are now given free hand to improve the very simple digit classification\n",
    "trained above. Your task is to use a **fully-connected** (no conv nets allowed\n",
    "in this part) neural network to train a classifier that achieves a validation\n",
    "error lower than $5\\%$.\n",
    "\n",
    "Apart from the restriction to fully-connected layers, you are allowed (and even encouraged) to use all other available features\n",
    "from the `pytorch` library.\n",
    "Helpful `pytorch` sub-modules are the [nn](https://pytorch.org/docs/stable/nn.html) module and the [optmizers](https://pytorch.org/docs/stable/optim.html) module. \n",
    "\n",
    "**Add your code below, do not overwrite what you have done above!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
